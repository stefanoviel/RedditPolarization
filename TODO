

Order stuff to embed by length 
Check the data what is there? Just links maybe? 
Path parametrised
No ripetizioni


TODO:
there is some stocasticity in the dimensionality reduction. for the final result it would probably require multiple runs. 
score filtering should differe every year (e.g. keep only the one with the 50% highest scores)



keep running on very small subset (stronger subsampling)
un altro test sul dimensionality reduction
* match clusters by maximizing the number of elements which interesect between the two
report of memory usage in each part of pipeline
go back to duckdb to read from parquest files



do we actually need a DB? 
It make the querying of the subreddit easier later. We will only use it to find which the corresponding information of a certain embedding
It should only contain the data we're going to embed. We don't want to copy everything. Create the parquet files with only the data you need. 