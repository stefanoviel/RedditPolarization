
TODO:
there is some stocasticity in the dimensionality reduction. for the final result it would probably require multiple runs. 
score filtering should differe every year (e.g. keep only the one with the 50% highest scores)
trustworthiness takes very long on small dataset (more than one minutes fro 500k embeddings). Testing done with this metrics will have to be run on a subset.
add min_df=2, ngram_range=(1, 2) in TF-IDF
put random seeds everywhere


DBCV discounted 
module load stack/.2024-05-silent  gcc/13.2.0 python_cuda/3.11.6

exam for chatgpt
input: clusters, posts, topics
sample some post from each cluster
create tests where each post is associated with 5 options of topics and you measure the accuracy
